# ğŸ¡ House Price Classifier: Expensive or Not?

This project builds an end-to-end machine learning pipeline to classify houses in Ames, Iowa, as **"expensive"** or **"not expensive"** based on 80+ features from the housing dataset. It's part of my final project for a machine learning course and demonstrates real-world model development, preprocessing, evaluation, and optimization.

---

## âœ¨ Overview

* ğŸ“‚ Dataset: Ames Housing Dataset (81 features)
* ğŸ“ Goal: Classify whether a house is "expensive" using numerical, ordinal, and categorical features
* ğŸš€ Techniques: Preprocessing pipelines, feature engineering, grid search, model comparison
* ğŸ”§ Tools: Scikit-learn, XGBoost, pandas, NumPy, matplotlib, seaborn

---

## ğŸ“† Project Structure

```
â”œâ”€â”€ data/                # housing dataset
â”œâ”€â”€ notebooks/           # Colab notebooks
â”‚   â””â”€â”€ final_model.ipynb
â”œâ”€â”€ images/              # Plots and visualizations
â”œâ”€â”€ README.md            # Project documentation (this file)
â”œâ”€â”€ requirements.txt     # List of dependencies
â””â”€â”€ LICENSE              # MIT License
```

---

## ğŸ“Š Features & Preprocessing

* **Numerical**: Scaled using `StandardScaler`
* **Categorical**: One-hot encoded
* **Ordinal**: Custom mappings (e.g., Quality: Po < Fa < TA < Gd < Ex)
* **Missing values**: Imputed using `SimpleImputer`
* **Feature selection**: Low-variance and irrelevant features dropped

---

## ğŸ¤– Models Evaluated

| Model         | Accuracy                   | Precision | Recall | F1 Score | Kappa |
| ------------- | -------------------------- | --------- | ------ | -------- | ----- |
| SVC           | âœ” Best Overall             | 	0.9	     |0.857143|	0.878049 | 0.858142
| Random Forest | Good balance               |  0.941176 |0.761905| 0.842105 |0.818784
| KNN           | Performs well with scaling |  0.853659 |0.833333| 0.843373 | 0.81743 

> SVC achieved the **best overall performance** across all metrics.

---

## ğŸ”¢ Metrics Used

* Accuracy, Precision, Recall, F1 Score
* Specificity & Balanced Accuracy
* Cohen's Kappa

---

## âš–ï¸ Why Not Just Use Price?

* Price alone isn't enough: it depends on **lot size, location, quality, finishes, and more**.
* Classification allows targeting for decision-making (e.g. "worth investing in?").

---

## ğŸ” Key Insights

* Feature preprocessing was **crucial** due to mixed data types (ordinal, nominal, numeric)
* Some models like GaussianNB performed well on recall but poorly on precision
* Grid search improved DecisionTree and XGBoost significantly
* Feature importance from tree-based models helped refine the pipeline

---

## ğŸš§ Limitations & Ideas

* Current features **lack semantics**: we ignore **text descriptions, images, or location proximity**
* **Genre/culture equivalents in housing** like neighborhood prestige aren't fully captured
* Could enhance with **clustering** or **NLP on descriptions** in future work

---

## ğŸš€ Run the Notebook

Open [`notebooks/final_model.ipynb`](notebooks/Copetition_pipeline_for_multiple_models.ipynb) to walk through the full pipeline:

* Preprocessing
* Model training & tuning
* Evaluation

---

## âœï¸ License

This project is licensed under the **MIT License**.

---

## ğŸš€ Author

**Hassan Merai(Fred)**
Data scientist passionate about real-world ML and automation. Connect with me:

* [LinkedIn](https://www.linkedin.com/in/Hassan-Merai-nickname-Fred)
* [Medium](https://medium.com/@Hassan-Merai)
* [GitHub](https://github.com/Hassan-Merai)

---

*"Teaching machines to spot an expensive house isn't just about numbers â€” it's about mimicking human judgment."*
